{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c430868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8319529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-194a9577ff1cef03\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-194a9577ff1cef03\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tensorboard magic, remember to set your host and port, then forward them\n",
    "# You can set reload data under settings to see data update as you run your model\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/tensorboard --host=10.128.10.14 --port=8008"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e6f1f7-efb8-426c-b4e2-7a38e61e71e9",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d8b5720-2e84-4bd5-8a7c-17116b57e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining hyperparameters\n",
    "batch_size = 32\n",
    "\n",
    "def apply_image_transformation(transformation_type=\"standard\", *args, **kwargs):\n",
    "    '''\n",
    "    Apply various image transformations based on the provided transformation_type.\n",
    "\n",
    "    Args:\n",
    "    transformation_type (str): The type of transformation to apply. Supported types are 'standard', 'resize', and 'channel'.\n",
    "    *args: Additional arguments based on the transformation type.\n",
    "    **kwargs: Additional keyword arguments for normalization parameters.\n",
    "\n",
    "    Returns:\n",
    "    transform: A composition of transformations to be applied to the input images.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If an unsupported transformation type is provided.\n",
    "\n",
    "    '''\n",
    "    if transformation_type == \"standard\":\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(0.5, 0.5)\n",
    "        ])\n",
    "        return transform\n",
    "\n",
    "    elif transformation_type == \"resize\":\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(args[0]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(kwargs['mean'], kwargs['std'])\n",
    "        ])\n",
    "        return transform\n",
    "\n",
    "    elif transformation_type == \"channel\":\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Grayscale(args[0]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=kwargs['mean'], std=kwargs['std'])\n",
    "        ])\n",
    "        return transform\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid transformation type. Supported types are 'normal', 'resize', and 'grayscale'.\")\n",
    "\n",
    "def create_loader(transform, batch_size=32):\n",
    "    '''\n",
    "    Create data loaders for training and testing using the provided transformation.\n",
    "\n",
    "    Args:\n",
    "    transform: The transformation to be applied to the dataset.\n",
    "    batch_size: The batch size of \n",
    "\n",
    "    Returns:\n",
    "    train_loader: DataLoader for the training dataset.\n",
    "    test_loader: DataLoader for the testing dataset.\n",
    "\n",
    "    This function imports the FashionMNIST dataset from the torchvision library and applies the provided transformation to the dataset. It then creates data loaders for both the training and testing datasets, considering the specified transformation and other default parameters such as the number of workers and batch size.\n",
    "    '''\n",
    "    # importing training and test sets from torchvision\n",
    "    train_dataset = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=False, transform=transform)\n",
    "    \n",
    "    # creating dataloaders\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, num_workers=2, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False, num_workers=2, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f798e893-9f7d-48f2-8e7f-34b96a4c3d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = apply_image_transformation('standard')\n",
    "basic_train_loader, basic_test_loader = create_loader(transform)\n",
    "\n",
    "transform = apply_image_transformation('standard')\n",
    "vgg_train_loader, vgg_test_loader = create_loader(transform)\n",
    "\n",
    "transform = apply_image_transformation('resize', 224, mean=(0.1307,), std=(0.3081,))\n",
    "vgg_resize_train_loader, vgg_resize_test_loader = create_loader(transform)\n",
    "\n",
    "transform = apply_image_transformation('channel', 3, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "vgg_channel_train_loader, vgg_channel_test_loader = create_loader(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcc8b90-ddf0-4d69-bcc1-c8dc69222099",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fba70826",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def train(model, criterion, optimizer, num_epochs, train_loader, test_loader, writer=None):\n",
    "  total_time_taken = 0.0\n",
    "  # Train the model\n",
    "  for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    for i, data in enumerate(tqdm(train_loader)):\n",
    "      # Get the inputs and labels\n",
    "      inputs, labels = data\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "      # Zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(inputs)\n",
    "\n",
    "      # Compute the loss\n",
    "      loss = criterion(outputs, labels)\n",
    "      train_loss += loss.item()\n",
    "\n",
    "      # Backward pass and optimize\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # Compute correct predictions\n",
    "      pred = outputs.argmax(dim=1, keepdim=True)\n",
    "      correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "      # Log training loss per 200 mini batches using TensorBoard\n",
    "      if writer:\n",
    "        if i % 200 == 199:\n",
    "          writer.add_scalar('Train Loss', train_loss/200, epoch * len(train_loader) + i)\n",
    "\n",
    "    total_time_taken += time.time() - start_time\n",
    "\n",
    "    # Compute train accuracy\n",
    "    train_accuracy = 100. * correct / len(train_loader.dataset)\n",
    "\n",
    "    # Log training loss per 200 mini batches using TensorBoard\n",
    "    if writer:\n",
    "      writer.add_scalar('Train Accuracy', train_accuracy, epoch)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    test_acc = []\n",
    "    with torch.no_grad():\n",
    "      for i, data in enumerate(test_loader):\n",
    "        # Get the inputs and labels\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Compute correct predictions\n",
    "        pred = outputs.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "        # Log training loss per 200 mini batches using TensorBoard\n",
    "        if writer:\n",
    "          if i % 200 == 199:\n",
    "            writer.add_scalar('Test Loss', train_loss/200, epoch * len(test_loader) + i)\n",
    "\n",
    "    # Compute test accuracy\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    test_acc.append(test_accuracy)\n",
    "\n",
    "    # Log training loss per 200 mini batches using TensorBoard\n",
    "    if writer:\n",
    "      writer.add_scalar('Test Accuracy', test_accuracy, epoch)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "  print(f'\\n\\nTotal Time Elapsed: {total_time_taken} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ef4fe-2ec7-40aa-a88b-d44c35815357",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "Rather than using a simple convolution layer, each convolution layer is transformed into a 3 layer architecture\n",
    "- Convolution Layer\n",
    "- Batch Normalization Layer\n",
    "- Activation Layer (LeakyReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cd02978-a97e-4132-9583-29d6ec9a4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0, stride=1):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.basicconv2d = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.basicconv2d(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11494bb4-9422-4956-b52d-dca7cdef1193",
   "metadata": {},
   "source": [
    "## Basic CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed6083ac-d021-43a8-b872-2ec7342f2a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicCNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(BasicCNN, self).__init__()\n",
    "    self.conv1 = BasicConv2d(1, 32, 3) # 32 1x3x3 filters with stride 1, pad 0\n",
    "    '''\n",
    "    Output size = (28 - 3 + 2*0)/1 + 1 = 26\n",
    "    Output volume = 32x26x26\n",
    "    '''\n",
    "    self.pool = nn.MaxPool2d(2, 2) # 2x2 filter with stride 2\n",
    "    '''\n",
    "    Output size = (26 - 2)/2 + 1 = 13\n",
    "    Output volume = 32x13x13\n",
    "    '''\n",
    "    self.fc1 = nn.Linear(32 * 13 * 13, 100)\n",
    "    self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.relu(self.conv1(x))\n",
    "    x = self.pool(x)\n",
    "    # Flatten the output of the last convolutional layer\n",
    "    x = x.view(-1, 32 * 13 * 13)\n",
    "    # Apply the fully connected layers with ReLU activation\n",
    "    x = torch.relu(self.fc1(x))\n",
    "    # Apply the last fully connected layer with softmax activation\n",
    "    x = self.fc2(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a636689-1a62-4fc1-a771-d3cd6ea49211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:09<00:00, 199.83it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.3786, Train Accuracy: 86.53%, Test Loss: 0.3063, Test Accuracy: 88.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:09<00:00, 205.86it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.2656, Train Accuracy: 90.21%, Test Loss: 0.2809, Test Accuracy: 89.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:09<00:00, 204.32it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.2233, Train Accuracy: 91.73%, Test Loss: 0.2718, Test Accuracy: 90.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:10<00:00, 185.46it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.1934, Train Accuracy: 92.95%, Test Loss: 0.2779, Test Accuracy: 90.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:09<00:00, 195.34it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.1713, Train Accuracy: 93.72%, Test Loss: 0.2768, Test Accuracy: 90.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:09<00:00, 206.10it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.1480, Train Accuracy: 94.46%, Test Loss: 0.2750, Test Accuracy: 91.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:09<00:00, 205.39it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.1318, Train Accuracy: 95.05%, Test Loss: 0.3067, Test Accuracy: 90.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:09<00:00, 199.71it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.1160, Train Accuracy: 95.68%, Test Loss: 0.3134, Test Accuracy: 91.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:09<00:00, 199.59it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.0994, Train Accuracy: 96.23%, Test Loss: 0.3406, Test Accuracy: 91.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:09<00:00, 202.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.0897, Train Accuracy: 96.65%, Test Loss: 0.3390, Test Accuracy: 90.37%\n",
      "\n",
      "\n",
      "Total Time Elapsed: 93.64534449577332 s\n"
     ]
    }
   ],
   "source": [
    "# defining hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "BasicCNNModel = BasicCNN().to(device)\n",
    "writer = SummaryWriter('logs/tensorboard/Basic/BasicModel')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(BasicCNNModel.parameters(), lr=learning_rate)\n",
    "\n",
    "train(BasicCNNModel, criterion, optimizer, num_epochs, basic_train_loader, basic_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b595ac3",
   "metadata": {},
   "source": [
    "## VGG Architecture\n",
    "The VGG architecture proposed in the original paper worked on an input image dimension of 224x224x3. Since the Fashion MNIST dataset are of 28x28x1 dimension, either the image has to be rescaled, or the architecture has to be modified to better fit our use case.\n",
    "\n",
    "### Original VGG16 Architecture\n",
    "For this implementation, we utilize the original VGG16 architecture with a few key modifications to accommodate the Fashion MNIST dataset. Initially, we resize the input image dimensions to 224x224x1 to align with the VGG model's input requirements. Additionally, we introduce small dropout layers after each fully connected layer. This measure helps prevent overfitting, considering that the original VGG model was primarily designed for the ImageNet dataset, which contains a significantly larger volume of images. \n",
    "s.\n",
    "![](https://media.geeksforgeeks.org/wp-content/uploads/20200219152207/new41.jpg)\n",
    "\n",
    "### Modified VGG Architecture\n",
    "Propsed architecture 1:\n",
    "- 2x Convulution + ReLU (28 x 28 x 64)\n",
    "- Max Pooling (14 x 14 x 64)\n",
    "- 2x Convolution + ReLU (14 x 14 x 128)\n",
    "- Max Pooling (7 x 7 x 128)\n",
    "- 3x Convolution + ReLU (7 x 7 x 256)\n",
    "- Max Pooling (3 x 3 x 256)\n",
    "- 3x Convolution + ReLU (3 x 3 x 512)\n",
    "- Max Pooling (1 x 1 x 512)\n",
    "\n",
    "Propsed architecture 1:\n",
    "- 2x Convulution + ReLU (28 x 28 x 64)\n",
    "- Max Pooling (14 x 14 x 64)\n",
    "- 2x Convolution + ReLU (14 x 14 x 128)\n",
    "- Max Pooling (7 x 7 x 128)\n",
    "- 3x Convolution + ReLU (7 x 7 x 256)\n",
    "- Max Pooling (3 x 3 x 256)\n",
    "\n",
    "Propsed architecture 1:\n",
    "- 2x Convulution + ReLU (28 x 28 x 64)\n",
    "- Max Pooling (14 x 14 x 64)\n",
    "- 2x Convolution + ReLU (14 x 14 x 128)\n",
    "- Max Pooling (7 x 7 x 128)\n",
    "\n",
    "Fully connected layer for all three architecture will follow a similar 3-FC layer as implemented in the original VGG16. However, the output dimension each layer have been scaled down accordingly to the nearest power 2.\n",
    "\n",
    "Example:\n",
    "- Proposed architecture 1: Scaled down from 7x7x512 to 1x1x512, by a factor of 49 => 4096/49 = 84 => 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b35b7b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(VGG16, self).__init__()\n",
    "    self.conv1_1 = BasicConv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "    self.conv1_2 = BasicConv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "\n",
    "    self.conv2_1 = BasicConv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "    self.conv2_2 = BasicConv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "\n",
    "    self.conv3_1 = BasicConv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "    self.conv3_2 = BasicConv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "    self.conv3_3 = BasicConv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "    self.conv4_1 = BasicConv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "    self.conv4_2 = BasicConv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "    self.conv4_3 = BasicConv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "\n",
    "    self.conv5_1 = BasicConv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "    self.conv5_2 = BasicConv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "    self.conv5_3 = BasicConv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "\n",
    "    self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.fc1 = nn.Linear(7 * 7 * 512, 4096)\n",
    "    self.fc2 = nn.Linear(4096, 4096)\n",
    "    self.fc3 = nn.Linear(4096, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.relu(self.conv1_1(x))\n",
    "    x = torch.relu(self.conv1_2(x))\n",
    "    x = self.maxpool(x)\n",
    "    x = torch.relu(self.conv2_1(x))\n",
    "    x = torch.relu(self.conv2_2(x))\n",
    "    x = self.maxpool(x)\n",
    "    x = torch.relu(self.conv3_1(x))\n",
    "    x = torch.relu(self.conv3_2(x))\n",
    "    x = torch.relu(self.conv3_3(x))\n",
    "    x = self.maxpool(x)\n",
    "    x = torch.relu(self.conv4_1(x))\n",
    "    x = torch.relu(self.conv4_2(x))\n",
    "    x = torch.relu(self.conv4_3(x))\n",
    "    x = self.maxpool(x)\n",
    "    x = torch.relu(self.conv5_1(x))\n",
    "    x = torch.relu(self.conv5_2(x))\n",
    "    x = torch.relu(self.conv5_3(x))\n",
    "    x = self.maxpool(x)\n",
    "    x = x.view(-1, 7 * 7 * 512)\n",
    "    x = torch.relu(self.fc1(x))\n",
    "    x = nn.functional.dropout(x, 0.2)\n",
    "    x = torch.relu(self.fc2(x))\n",
    "    x = nn.functional.dropout(x, 0.2)\n",
    "    x = self.fc3(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4829d047-4a3c-40c2-974c-1939db56382f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [06:17<00:00,  4.97it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.8073, Train Accuracy: 74.27%, Test Loss: 0.5228, Test Accuracy: 81.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [06:17<00:00,  4.97it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.4148, Train Accuracy: 85.03%, Test Loss: 0.3686, Test Accuracy: 86.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [06:17<00:00,  4.97it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.3344, Train Accuracy: 88.12%, Test Loss: 0.3222, Test Accuracy: 88.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [06:17<00:00,  4.97it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.2860, Train Accuracy: 89.76%, Test Loss: 0.2860, Test Accuracy: 89.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [06:17<00:00,  4.97it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.2539, Train Accuracy: 91.00%, Test Loss: 0.2698, Test Accuracy: 90.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [06:17<00:00,  4.97it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.2284, Train Accuracy: 91.86%, Test Loss: 0.2573, Test Accuracy: 90.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [06:17<00:00,  4.97it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.2069, Train Accuracy: 92.59%, Test Loss: 0.2414, Test Accuracy: 92.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [06:17<00:00,  4.97it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.1859, Train Accuracy: 93.34%, Test Loss: 0.2406, Test Accuracy: 91.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [06:17<00:00,  4.97it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.1733, Train Accuracy: 93.88%, Test Loss: 0.2317, Test Accuracy: 92.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [06:17<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.1542, Train Accuracy: 94.55%, Test Loss: 0.2214, Test Accuracy: 92.29%\n",
      "\n",
      "\n",
      "Total Time Elapsed: 3773.766048669815 s\n"
     ]
    }
   ],
   "source": [
    "# defining hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "VGGModel = VGG16().to(device)\n",
    "writer = SummaryWriter('logs/tensorboard/VGG/VGG16')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(VGGModel.parameters(), lr=learning_rate)\n",
    "\n",
    "train(VGGModel, criterion, optimizer, num_epochs, vgg_resize_train_loader, vgg_resize_test_loader, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80a98c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedVGG_1(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ModifiedVGG_1, self).__init__()\n",
    "    self.conv1_1 = BasicConv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "    self.conv1_2 = BasicConv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "\n",
    "    self.conv2_1 = BasicConv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "    self.conv2_2 = BasicConv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "\n",
    "    self.conv3_1 = BasicConv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "    self.conv3_2 = BasicConv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "    self.conv3_3 = BasicConv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "    self.conv4_1 = BasicConv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "    self.conv4_2 = BasicConv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "    self.conv4_3 = BasicConv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "\n",
    "    self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.fc1 = nn.Linear(1 * 1 * 512, 64)\n",
    "    self.fc2 = nn.Linear(64, 64)\n",
    "    self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.relu(self.conv1_1(x))\n",
    "    x = torch.relu(self.conv1_2(x))\n",
    "    x = self.maxpool(x)\n",
    "    x = torch.relu(self.conv2_1(x))\n",
    "    x = torch.relu(self.conv2_2(x))\n",
    "    x = self.maxpool(x)\n",
    "    x = torch.relu(self.conv3_1(x))\n",
    "    x = torch.relu(self.conv3_2(x))\n",
    "    x = torch.relu(self.conv3_3(x))\n",
    "    x = self.maxpool(x)\n",
    "    x = torch.relu(self.conv4_1(x))\n",
    "    x = torch.relu(self.conv4_2(x))\n",
    "    x = torch.relu(self.conv4_3(x))\n",
    "    x = self.maxpool(x)\n",
    "    x = x.view(-1, 1 * 1 * 512)\n",
    "    x = torch.relu(self.fc1(x))\n",
    "    x = torch.relu(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0724412a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:20<00:00, 92.52it/s] \n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.4438, Train Accuracy: 83.88%, Test Loss: 0.2927, Test Accuracy: 89.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:18<00:00, 98.77it/s] \n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.2724, Train Accuracy: 90.34%, Test Loss: 0.3176, Test Accuracy: 88.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:18<00:00, 99.00it/s] \n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.2288, Train Accuracy: 91.85%, Test Loss: 0.2636, Test Accuracy: 91.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:19<00:00, 98.56it/s] \n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.2006, Train Accuracy: 92.82%, Test Loss: 0.2374, Test Accuracy: 91.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:18<00:00, 98.75it/s] \n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.1762, Train Accuracy: 93.77%, Test Loss: 0.2245, Test Accuracy: 91.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:19<00:00, 94.70it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.1537, Train Accuracy: 94.48%, Test Loss: 0.2090, Test Accuracy: 92.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:19<00:00, 96.12it/s] \n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.1338, Train Accuracy: 95.24%, Test Loss: 0.2190, Test Accuracy: 92.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:18<00:00, 98.77it/s] \n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.1179, Train Accuracy: 95.79%, Test Loss: 0.2233, Test Accuracy: 92.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:19<00:00, 97.53it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.0999, Train Accuracy: 96.42%, Test Loss: 0.2167, Test Accuracy: 93.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:19<00:00, 98.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.0873, Train Accuracy: 96.92%, Test Loss: 0.2436, Test Accuracy: 92.82%\n",
      "\n",
      "\n",
      "Total Time Elapsed: 192.8491780757904 s\n"
     ]
    }
   ],
   "source": [
    "# defining hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "ModifiedVGGModel_1 = ModifiedVGG_1().to(device)\n",
    "writer = SummaryWriter('logs/tensorboard/VGG/ModifiedVGG_1')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ModifiedVGGModel_1.parameters(), lr=learning_rate)\n",
    "\n",
    "train(ModifiedVGGModel_1, criterion, optimizer, num_epochs, vgg_train_loader, vgg_test_loader, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65b38898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedVGG_2(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ModifiedVGG_2, self).__init__()\n",
    "    self.conv1_1 = BasicConv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "    self.conv1_2 = BasicConv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "\n",
    "    self.conv2_1 = BasicConv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "    self.conv2_2 = BasicConv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "\n",
    "    self.conv3_1 = BasicConv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "    self.conv3_2 = BasicConv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "    self.conv3_3 = BasicConv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "    self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.fc1 = nn.Linear(3 * 3 * 256, 256)\n",
    "    self.fc2 = nn.Linear(256, 256)\n",
    "    self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.relu(self.conv1_1(x))\n",
    "    x = torch.relu(self.conv1_2(x))\n",
    "    x = self.maxpool(x)\n",
    "    x = torch.relu(self.conv2_1(x))\n",
    "    x = torch.relu(self.conv2_2(x))\n",
    "    x = self.maxpool(x)\n",
    "    x = torch.relu(self.conv3_1(x))\n",
    "    x = torch.relu(self.conv3_2(x))\n",
    "    x = torch.relu(self.conv3_3(x))\n",
    "    x = self.maxpool(x)\n",
    "    x = x.view(-1, 3 * 3 * 256)\n",
    "    x = torch.relu(self.fc1(x))\n",
    "    x = torch.relu(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26276989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:14<00:00, 125.80it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.3900, Train Accuracy: 85.75%, Test Loss: 0.3224, Test Accuracy: 88.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:15<00:00, 120.19it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.2532, Train Accuracy: 90.67%, Test Loss: 0.2738, Test Accuracy: 90.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:16<00:00, 111.69it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.2121, Train Accuracy: 92.18%, Test Loss: 0.2744, Test Accuracy: 90.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:14<00:00, 125.13it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.1816, Train Accuracy: 93.39%, Test Loss: 0.2278, Test Accuracy: 92.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:15<00:00, 124.98it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.1593, Train Accuracy: 94.23%, Test Loss: 0.2174, Test Accuracy: 92.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:14<00:00, 125.04it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.1373, Train Accuracy: 94.97%, Test Loss: 0.2131, Test Accuracy: 92.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:15<00:00, 122.34it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.1162, Train Accuracy: 95.64%, Test Loss: 0.2342, Test Accuracy: 92.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:14<00:00, 125.54it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.1013, Train Accuracy: 96.23%, Test Loss: 0.2325, Test Accuracy: 93.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:15<00:00, 121.09it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.0867, Train Accuracy: 96.81%, Test Loss: 0.2502, Test Accuracy: 92.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:15<00:00, 122.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.0729, Train Accuracy: 97.37%, Test Loss: 0.2629, Test Accuracy: 93.18%\n",
      "\n",
      "\n",
      "Total Time Elapsed: 153.30515146255493 s\n"
     ]
    }
   ],
   "source": [
    "# defining hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "ModifiedVGGModel_2 = ModifiedVGG_2().to(device)\n",
    "writer = SummaryWriter('logs/tensorboard/VGG/ModifiedVGG_2')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ModifiedVGGModel_2.parameters(), lr=learning_rate)\n",
    "\n",
    "train(ModifiedVGGModel_2, criterion, optimizer, num_epochs, vgg_train_loader, vgg_test_loader, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edc039b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedVGG_3(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ModifiedVGG_3, self).__init__()\n",
    "    self.conv1_1 = BasicConv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "    self.conv1_2 = BasicConv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "\n",
    "    self.conv2_1 = BasicConv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "    self.conv2_2 = BasicConv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "\n",
    "    self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.fc1 = nn.Linear(7 * 7 * 128, 1024)\n",
    "    self.fc2 = nn.Linear(1024, 1024)\n",
    "    self.fc3 = nn.Linear(1024, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.relu(self.conv1_1(x))\n",
    "    x = torch.relu(self.conv1_2(x))\n",
    "    x = self.maxpool(x)\n",
    "    x = torch.relu(self.conv2_1(x))\n",
    "    x = torch.relu(self.conv2_2(x))\n",
    "    x = self.maxpool(x)\n",
    "    x = x.view(-1, 7 * 7 * 128)\n",
    "    x = torch.relu(self.fc1(x))\n",
    "    x = torch.relu(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b835e5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:13<00:00, 142.48it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.4070, Train Accuracy: 85.28%, Test Loss: 0.2780, Test Accuracy: 90.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:12<00:00, 150.94it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.2472, Train Accuracy: 91.08%, Test Loss: 0.2366, Test Accuracy: 91.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:12<00:00, 150.67it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.2037, Train Accuracy: 92.52%, Test Loss: 0.2273, Test Accuracy: 91.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:12<00:00, 150.32it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.1722, Train Accuracy: 93.73%, Test Loss: 0.2201, Test Accuracy: 92.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:12<00:00, 146.34it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.1480, Train Accuracy: 94.58%, Test Loss: 0.2028, Test Accuracy: 93.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:12<00:00, 151.42it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.1229, Train Accuracy: 95.55%, Test Loss: 0.2205, Test Accuracy: 93.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:12<00:00, 151.61it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.0982, Train Accuracy: 96.28%, Test Loss: 0.2448, Test Accuracy: 93.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:13<00:00, 135.08it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.0817, Train Accuracy: 96.99%, Test Loss: 0.2807, Test Accuracy: 92.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:12<00:00, 146.78it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.0659, Train Accuracy: 97.60%, Test Loss: 0.3046, Test Accuracy: 92.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:14<00:00, 131.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.0569, Train Accuracy: 97.97%, Test Loss: 0.2991, Test Accuracy: 92.85%\n",
      "\n",
      "\n",
      "Total Time Elapsed: 128.95099425315857 s\n"
     ]
    }
   ],
   "source": [
    "# defining hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "ModifiedVGGModel_3 = ModifiedVGG_3().to(device)\n",
    "writer = SummaryWriter('logs/tensorboard/VGG/ModifiedVGG_3')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ModifiedVGGModel_3.parameters(), lr=learning_rate)\n",
    "\n",
    "train(ModifiedVGGModel_3, criterion, optimizer, num_epochs, vgg_train_loader, vgg_test_loader, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df22431-8690-4d54-84b3-10c711fa25a1",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56e0d28-8287-4d9e-b047-3d438ae8691b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dfcfd41-5075-4b21-8277-07709a633ea7",
   "metadata": {},
   "source": [
    "# Inceptionv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24edd90d-445c-446e-8069-541e6de8b8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d37323c-a233-49c8-b34d-7e112aeb6210",
   "metadata": {},
   "source": [
    "---\n",
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44831a6-3586-44b3-a29a-ecdb792e054b",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02c1e9ac-0891-4046-9781-fc4be87ae3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/UG/chua0994/.cache/torch/hub/pytorch_vision_v0.7.0\n",
      "100%|██████████| 1875/1875 [05:50<00:00,  5.35it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6096, Train Accuracy: 80.42%, Test Loss: 0.3625, Test Accuracy: 86.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [05:49<00:00,  5.36it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.3564, Train Accuracy: 87.19%, Test Loss: 0.3189, Test Accuracy: 87.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [05:49<00:00,  5.36it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.5475, Train Accuracy: 85.10%, Test Loss: 0.3259, Test Accuracy: 87.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [05:49<00:00,  5.36it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.3148, Train Accuracy: 88.45%, Test Loss: 0.2806, Test Accuracy: 89.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [05:49<00:00,  5.36it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.2912, Train Accuracy: 89.52%, Test Loss: 0.2855, Test Accuracy: 89.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [05:49<00:00,  5.36it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.2798, Train Accuracy: 89.83%, Test Loss: 0.3071, Test Accuracy: 88.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [05:49<00:00,  5.36it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.2961, Train Accuracy: 89.42%, Test Loss: 0.2720, Test Accuracy: 90.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [05:49<00:00,  5.36it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.2703, Train Accuracy: 90.08%, Test Loss: 0.2730, Test Accuracy: 90.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [05:49<00:00,  5.36it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.2487, Train Accuracy: 90.89%, Test Loss: 0.2920, Test Accuracy: 89.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [05:49<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.2446, Train Accuracy: 90.99%, Test Loss: 0.2705, Test Accuracy: 90.38%\n",
      "\n",
      "\n",
      "Total Time Elapsed: 3498.5282509326935 s\n"
     ]
    }
   ],
   "source": [
    "PretrainedVGGModel = torch.hub.load('pytorch/vision:v0.7.0', 'vgg16', pretrained=True).to(device)\n",
    "writer = SummaryWriter('logs/tensorboard/VGG/PretrainedVGG')\n",
    "\n",
    "# defining hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(PretrainedVGGModel.parameters(), lr=learning_rate)\n",
    "\n",
    "train(PretrainedVGGModel, criterion, optimizer, num_epochs, vgg_channel_train_loader, vgg_channel_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d4095-b7d8-482a-bc80-287f2bcd5032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
